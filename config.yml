## Configuration
#
# Data files and directories
#
directories:
  papers:   _PAPERS       # Where PDF documents reside.
  json:     _JSON         # Where the parsed JSON files are stored.
  mcq:      _MCQ          # Where your MCQ processing code will read/write.
  results: _RESULTS       # Where final results will be stored.

# Model Defaults
#
timeout:    45            # model query timeout

# Model 
# temperature, base model, tokenizer applied to whatever model is specified
# on the command line. Model name below is used if no model is specified on 
# the comment line.
#
# 
  #name:        "cafe:meta-llama/Llama-3.3-70B-Instruct"
  #name:        "cafe:llama31-405b-fp8"
  #name:        "alcf:argonne-private/AuroraGPT-Tulu3-SFT-0125"
  #name:        "alcf:argonne-private/AuroraGPT-IT-v4-0125"
  #name:        "alcf:argonne-private/AuroraGPT-7B"
  #name:        "alcf:meta-llama/Meta-Llama-3-70B-Instruct"
  #name:        "pb:mistralai/Mistral-7B-Instruct-v0.3"
  #name:        "openai:gpt-4"
  #name:        "pb:argonne-private/Llama2-70B"
  #name:        "local:qwen2.5-7b-instruct-1m"
  #name:        "local:qwen2.5-7b-instruct-1m"
  #name:        "local:deepseek-r1-distill-llama-8b"
  #name:         "test:test"

# this model is the default for Model-A for score_answers.py
#
model:
  name:         "argo:gpt4"
  temperature:  0.7
  baseModel:   "None"
  Tokenizer:   "None"

# model-B for score_answers.py
# model-B also the model whose knowledge is tested w.r.t. factoid nuggets
#         (note Temperature=0 is necessary)

model_b:
  name:        "argo:gpt4o"
  temperature:   0.0
  baseModel:    "None"
  Tokenizer:    "None"

#model_c:
  #name:        "alcf:meta-llama/Meta-Llama-3-8B-Instruct"
  #temperature : 0.7
  #baseModel:   "None"
  #Tokenizer:   "None"

#model_d:
  #name:        "alcf:mistralai/Mistral-7B-Instruct-v0.3"
  #temperature:  0.7
  #baseModel:   "None"
  #Tokenizer:   "None"

# Argo API configuration
argo:
  username_file: secrets.yml  # Path to secrets file containing Argo credentials


# Other MCQ-related parameters

quality:
  minScore:         7
  chunkSize:     1000  # For generate_mcqs chunking json files for mcqs
  save_interval:   50  # For parallel_generate_answers.py and parallel_score_answers- how
                       # many QA pairs to process before updating resutls file)
  defaultThreads:   4  # default n-way parallel, can override with -p/--parallel
 

####### ABOUT CHANGING THESE PROMPTS #######
## NOTE: Python's str.format() treats any single braces {...} as placeholders.
# If you need a literal brace in your prompt (e.g., "\[ \boxed{3} \]"),
# you must double them like "\[ \boxed{{3}} \]". 
# ALSO: avoid putting placeholders in single quotes, as that can trigger KeyError.
#
# ----------------------------------------------------------------
#
# Prompts for generate_mcqs.py
# 
# Iteration with model to generate MCQ Q/A pairs
#
#
## Prompts for generate_mcqs.py
# 
# Iteration with model to generate MCQ Q/A pairs
#

prompts:

  # Step 1
  system_message: |
    You are a helpful assistant that summarizes text in bullet points and expands on them using your broader knowledge. Name this result 'augmented_chunk'.
  user_message: |
    Given the following chunk of text, please:

    1. Summarize the text in bullet points.
    2. Expand on the summary using your parametric knowledge to create a 80-100 word synopsis of the chunk.

    Chunk:
    {chunk}

    Return the result as a JSON object with key 'augmented_chunk' and the value should be your bullet points and comments.  The comments should be no longer than 100 words and should not include your reasoning narrative.

  # Step 4: Atomic fact extraction (for identifying novel facts)
  fact_extraction_system: |
    You are a precise fact extractor that identifies atomic, verifiable factual statements in scientific text.
    An atomic fact is a single, standalone claim that can be verified as true or false.
    Focus on extracting specific claims, findings, methodologies, and results.
    Return your output in a structured JSON format.

  fact_extraction_user: |
    Extract atomic factual statements expressed or implied in this text:

    {chunk}

    For each fact, provide:
    1. The specific claim as a single complete sentence
    2. The text span it was derived from (exact quote from the source)
    3. Your confidence (0.0-1.0) that this is a factual statement

    Return as a JSON array of objects with these exact keys:
    - "claim": The specific factual statement
    - "span": The source text span this is derived from
    - "confidence": Your confidence score (0.0-1.0)

    IMPORTANT GUIDELINES:
    - Extract 3-7 key facts from this text
    - Focus on specific, technical claims rather than general statements
    - Aim for high precision - only include clearly factual statements
    - Return valid JSON that can be parsed directly

  # For fact deduplication and similarity comparison
  fact_comparison_system: |
    You are a precise semantic comparison assistant. Compare two factual statements and determine their semantic similarity.
    Return a JSON object with two fields:
    - similarity_score: float between 0 and 1
    - reasoning: brief explanation of the similarity or differences

  fact_comparison_user: |
    Compare these two factual statements and determine if they express the same information:
    
    Fact 1: {fact1}
    Fact 2: {fact2}
    
    Return a similarity score (0-1) where:
    1.0 = identical meaning
    0.8-0.99 = same core fact with minor differences
    0.5-0.79 = partially overlapping information
    0.0-0.49 = different facts

  # Step 5: Fact validation (to assess if facts are actually valid claims)
  # Step 5: Fact validation (to assess if facts are actually valid claims)
  fact_validation_system: |
    You are a fact-checking assistant that assesses the validity of factual claims.
    You evaluate how precise, accurate, and well-supported each claim is based on the provided evidence.
    You provide clear confidence scores based on factual validity.
    Return your assessment in a structured JSON format.

  fact_validation_user: |
    Carefully assess each of the following factual claims extracted from a scientific text.
    For each claim, evaluate:
    1. Is it a precise, specific statement that could be verified?
    2. Is it properly supported by the provided span of text?
    3. Is it formulated as a clear, standalone fact?

    Claims to evaluate:
    {claims_json}

    For each claim, provide:
    - Revised claim text (if needed for clarity/precision)
    - Adjusted confidence score (0.0-1.0)
    - Brief reason for your assessment

    Return as a JSON array maintaining the same structure as the input, but with potentially revised claims and confidence scores.
  # Step 2
  system_message_2: |
    You are a helpful assistant that generates exactly ONE multiple-choice question based on the provided text (augmented_chunk). The question must have 5 possible answers, numbered 1 to 5. Exactly one of these 5 choices is correct. Mark the correct choice with '(*)' at the end for later grading.
  user_message_2: |
    Below is some content called augmented_chunk.
    Please:
    1) Create exactly one multiple-choice question that can be answered by the augmented_chunk.
    2) Provide five distinct options (1 to 5) as answers.
    3) Mark the correct answer with '(*)' at the end of that particular option.

    Constraints:
    - The question and answers must be self-contained and understandable without referencing the chunk.
    - Do not mention 'chunk' or 'augmented_chunk' or 'article' or 'study' in the final output.

    augmented_chunk:
    {augmented_chunk}

  # Step 33
  system_message_3: |
    You are a helpful assistant that evaluates how well an answer matches the question in context of the augmented_chunk. Return your evaluation strictly as valid JSON with no additional text, with precisely three key-value pairs: answer, score, and comment. No additional formatting or text should be included, just these three key-value pairs in valid JSON format.
  user_message_3: |
    augmented_chunk:
    {augmented_chunk}

    question:
    {generated_question}

    Please provide, in JSON format:
    1. An appropriate answer to the multiple-choice question above. Report in JSON as the value for the key 'answer'.
    2. A single integer 'score' from 1 to 10 for how well the answer addresses the question based on the augmented_chunk. Report in JSON as the value for the key 'score'.
    3. A brief comment explaining why this answer is correct. Report in JSON as the value for the key 'comment'.

    Return a valid JSON object containing exactly three keys: 'answer', 'score', and 'comment'.

#
# Prompts for score_answers.py
#
scoring_prompts:
  main_system: "You are a strict grader. Respond with only the number."
  main_prompt: |
    You are a strict grader.

    Question: {question}
    Reference Answer: {reference_answer}
    User's Answer: {user_answer}

    On a scale of 1 to 10 (10 = exactly matches the reference answer, 1 = completely incorrect), provide ONLY the numeric score that reflects how well the User's Answer matches the Reference Answer. Provide just a number.
  fallback_system: "Extract a single number"
  fallback_prompt: |
    Extract a final answer from this user response. Sometimes this appears at the end after the words "**Final Answer**", enclosed in the characters "[ \\boxed{{" and "}} ]".
    For example, "[ \\boxed{{3}} ]" for the answer "3".

    Here is the user response:
    {user_answer}

#
# Prompts for generate_nugget.py
#
nugget_prompts:
  # Step 1: Metadata extraction
  metadata_system: |
    You are a helpful assistant that extracts paper metadata. 
    Focus on finding the exact paper identifiers, title, and first author.
    Look specifically for arXiv IDs, DOIs, or other unique identifiers.
    Return the information in a specific JSON format.

  metadata_user: |
    From this text, extract:
    1. Any paper identifiers (arXiv ID, DOI) that appear in the text
    2. The complete paper title
    3. The first author's full name

    Return as a JSON object with "identifiers" (array), "title", and "first_author".
    If you cannot find any field with certainty, use null.

    Text:
    {chunk}

  # Step 2: DOI lookup
  doi_system: |
    You are a helpful assistant that finds DOIs for academic papers.
    You have access to academic paper databases.
    Only return a DOI if you are certain it matches both the title and first author exactly.

  doi_user: |
    Find the exact DOI for this academic paper:
    Title: {title}
    First Author: {first_author}

    The paper appears to be about pervasive machine learning for HPC.
    Return ONLY the DOI string. If you cannot find an exact match, return null.

  # Step 3: Augmented chunk generation (same as generate_mcqs.py Step 1)
  system_message: |
    You are a helpful assistant that summarizes text in bullet points and expands on them using your broader knowledge. Name this result 'augmented_chunk'.

  user_message: |
    Given the following chunk of text, please:

    1. Summarize the text in bullet points.
    2. Expand on the summary using your parametric knowledge to create a 80-100 word synopsis of the chunk.

    Chunk:
    {chunk}

    Return the result as a JSON object with key 'augmented_chunk' and the value should be your bullet points and comments.  The comments should be no longer than 100 words and should not include your reasoning narrative.

