#
## Configuration
#
# Data files and directories
#
directories:
  papers:   _PAPERS       # Where PDF documents reside.
  json:     _JSON         # Where the parsed JSON files are stored.
  mcq:      _MCQ          # Where your MCQ processing code will read/write.
  results: _RESULTS       # Where final results will be stored.

# Model Defaults
# Prompts (system and user messges)
# Generate_mcq settings

# Model 
# temperature, base model, tokenizer applied to whatever model is specified
# on the command line. Model name below is used if no model is specified on 
# the comment line.
#
# this model will also be default for Model-A for score_answers.py
# 

model:
  name:        "local:deepseek-r1-distill-llama-8b"
  temperature:  0.7
  baseModel:   "None"
  Tokenizer:   "None"

# model-B for score_answers.py

model_b:
  name:        "local:deepseek-r1-distill-llama-8b"
  temperature:  0.7
  baseModel:   "None"
  Tokenizer:   "None"


# Other MCQ-related parameters

quality:
  minScore:    7
  chunkSize:   1000


####### ABOUT CHANGING THESE PROMPTS #######
## NOTE: Python's str.format() treats any single braces {...} as placeholders.
# If you need a literal brace in your prompt (e.g., "\[ \boxed{3} \]"),
# you must double them like "\[ \boxed{{3}} \]". 
# ALSO: avoid putting placeholders in single quotes, as that can trigger KeyError.
#
# ----------------------------------------------------------------
#
# Prompts for generate_mcqs.py
# 
# Iteration with model to generate MCQ Q/A pairs
#

prompts:

  # Step 1
  system_message: |
    You are a helpful assistant that summarizes text in bullet points and expands on them using your broader knowledge. Name this result 'augmented_chunk'.
  user_message: |
    Given the following chunk of text, please:

    1. Summarize the text in bullet points.
    2. Expand on the summary using your parametric knowledge.

    Chunk:
    {chunk}

    Return the result as plain text labeled 'augmented_chunk:' at the start.
    
  # Step 2
  system_message_2: |
    You are a helpful assistant that generates exactly ONE multiple-choice question based on the provided text (augmented_chunk). The question must have 5 possible answers, numbered 1 to 5. Exactly one of these 5 choices is correct. Mark the correct choice with '(*)' at the end for later grading.
  user_message_2: |
    Below is some content called augmented_chunk.
    Please:
    1) Create exactly one multiple-choice question that can be answered by the augmented_chunk.
    2) Provide five distinct options (1 to 5) as answers.
    3) Mark the correct answer with '(*)' at the end of that particular option.

    Constraints:
    - The question and answers must be self-contained and understandable without referencing the chunk.
    - Do not mention 'chunk' or 'augmented_chunk' or 'article' or 'study' in the final output.

    augmented_chunk:
    {augmented_chunk}

  # Step 33
  system_message_3: |
    You are a helpful assistant that evaluates how well an answer matches the question in context of the augmented_chunk. Return your evaluation strictly as valid JSON with no additional text, with precisely three key-value pairs: answer, score, and comment. No additional formatting or text shoud be included, just these three key-value pairs in valid JSON format.
  user_message_3: |
    augmented_chunk:
    {augmented_chunk}

    question:
    {generated_question}

    Please provide, in JSON format:
    1. An appropriate answer to the multiple-choice question above.
    2. A single integer 'score' from 1 to 10 for how well the answer addresses the question based on the augmented_chunk.
    3. A brief comment explaining why this answer is correct.

    Output must be valid JSON with the three key-value pairs of answer, score, and comment.

#
# ----------------------------------------------------------------
# Prompts for score_answers.py
# 
# Iteration with model to generate MCQ Q/A pairs
#
# (see notes above regarding format of prompts w.r.t. the 
# use of single quotes or curly brackets
#
scoring_prompts:
  main_system: "You are a strict grader. Respond with only the number."
  main_prompt: |
    You are a strict grader.

    Question: {question}
    Reference Answer: {reference_answer}
    User's Answer: {user_answer}

    On a scale of 1 to 10 (10 = exactly matches the reference answer,
    1 = completely incorrect), provide ONLY the numeric score that reflects
    how well the User's Answer matches the Reference Answer. Provide just a number.
    No extra text. No explanation. No formatting.

  fallback_system: "Extract a single number"
  fallback_prompt: |
    Extract a final answer from this user response. Sometimes this appears at the end
    after the words "**Final Answer**", enclosed in the characters "[ \\boxed{{" and "}} ]".
    For example, "[ \\boxed{{3}} ]" for the answer "3".

    Here is the user response:
    {user_answer}

